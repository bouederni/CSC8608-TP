# Exercice 1 : Comprendre la Matrice et Instrumenter l'Environnement (Exploration de Gymnasium)

## Question 1.b.
![1b.gif](img/1b.gif)
Ex√©cution : 
```
(.venv) spipo@DESKTOP-124UU29:~/Workspace/3-CSC8608-TP/TP5$ python random_agent.py 
/home/spipo/Workspace/3-CSC8608-TP/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import resource_stream, resource_exists
Espace d'observation (Capteurs) : Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.
  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.
  1.         1.       ], (8,), float32)
Espace d'action (Moteurs) : Discrete(4)

--- RAPPORT DE VOL ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -98.98 points
Allumages moteur principal : 17
Allumages moteurs lat√©raux : 41
Dur√©e du vol : 78 frames
Vid√©o de la t√©l√©m√©trie sauvegard√©e sous 'random_agent.gif'
```

## Question 2.b.

![2b.gif](img/2b.gif)

La r√©compense totale est inf√©rieure √† l'exercice pr√©c√©dent. Visiblement, l'IA √©vite de s'√©craser comme dans la derni√®re simulation, mais part totalement √† droite.

ep_raw_mean semble avoir oscill√© entre 0 et -20.

```
--- RAPPORT DE VOL PPO ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -107.06 points
Allumages moteur principal : 198
Allumages moteurs lat√©raux : 138
Dur√©e du vol : 348 frames
```

# Exercice 3 : L'Art du Reward Engineering (Wrappers et Hacking)

## Question 3.b.

![3b.gif](img/3b.gif)

```
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -115.41 points
Allumages moteur principal : 0
Allumages moteurs lat√©raux : 77
Dur√©e du vol : 77 frames
```

L‚Äôagent adopte une strat√©gie d‚Äô√©vitement total du moteur principal. Il n‚Äôallume jamais la pouss√©e verticale et laisse le module chuter ou d√©river passivement, utilisant au mieux les propulseurs lat√©raux peu p√©nalis√©s. Math√©matiquement, la r√©compense cumul√©e $( R = \sum_t r_t )$ est domin√©e par la p√©nalit√© de ‚àí50 associ√©e au moteur principal, bien sup√©rieure aux faibles r√©compenses d‚Äôun atterrissage r√©ussi. Toute trajectoire impliquant une seule activation devient sous‚Äëoptimale. L‚Äôoptimum consiste donc √† minimiser l‚Äôesp√©rance de p√©nalit√© imm√©diate, m√™me si cela r√©duit la probabilit√© de succ√®s. Logiquement, l‚Äôagent maximise la fonction modifi√©e, pas l‚Äôobjectif r√©el d‚Äôatterrissage s√ªr.

# 

## Question 4.b.

```
--- RAPPORT DE VOL PPO (GRAVIT√â MODIFI√âE) ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -78.40 points
Allumages moteur principal : 118
Allumages moteurs lat√©raux : 310
Dur√©e du vol : 435 frames
```

![4b.gif](img/4b.gif)

Ici, le vaisseau commence par s'√©lever un peu par rapport au point de d√©part et corrige son thruster bas pour se baisser doucement, mais il se d√©centre comme dans les derniers essais. 

# Exercice 5 : Bilan Ing√©nieur : Le d√©fi du Sim-to-Real

## Question 5.a.

Pour rendre l‚Äôagent robuste aux variations de gravit√© et de vent sans entra√Æner un mod√®le distinct par lune, une premi√®re strat√©gie consiste √† utiliser la domain randomization : √† chaque √©pisode, on √©chantillonne al√©atoirement la gravit√©, la force du vent ou la masse du module dans des plages r√©alistes. L‚Äôagent apprend ainsi une politique qui maximise l‚Äôesp√©rance de r√©compense sur une distribution d‚Äôenvironnements, et non sur une physique unique, ce qui limite le surapprentissage et am√©liore la g√©n√©ralisation.

Une deuxi√®me approche consiste √† enrichir l‚Äôespace d‚Äôobservation en fournissant explicitement les param√®tres physiques (gravit√© estim√©e, intensit√© du vent) en entr√©e du r√©seau. Le mod√®le apprend alors une politique conditionnelle capable d‚Äôadapter dynamiquement ses actions au contexte courant, sans multiplier les mod√®les ni modifier l‚Äôalgorithme PPO.